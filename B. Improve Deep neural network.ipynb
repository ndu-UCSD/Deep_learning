{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve Deep neural network\n",
    "\n",
    "     - Week 5\n",
    "     Setting up your Machine Learning Application\n",
    "     a) Data break into : Training set -> dev set (hold out, cross validation, development) -> test set: 70%/30%:0 or 60%/20%/20%            depend on the data size\n",
    "     b) Make sure dev and test set from same distribution\n",
    "     c) test set for unbaised performance evaluation\n",
    "     d) High bias - under fit; high variance - overfit. Low bias + low variance is what we expect\n",
    "     e) bays error - human error, compare to training set error and dev set error, to decide if the model is useful\n",
    "     f) High bias? (Y) -> bigger network, training longer, NN architecture search -> High bias? (N) -> High variance? (Y) -> more data,  regularization to reduce overfitting -> High variance (N) -> Done\n",
    "\n",
    "     Regularizing your neural network\n",
    "     g) Add Regularization to logistic regression: L2 regularization  add (lambda/2m)*||W||^2 to the cost function\n",
    "     h) lambda is the regularization parameter\n",
    "     i) large lambda set W[L] close to 0, which zeroing out some of the neurons to be inactive\n",
    "     j) Using tanh function is much less possible to overfit\n",
    "     k) Dropout function: killing random nodes. Inverted dropout, apply only to layers that have over fitting problem\n",
    "     l) Downside of dropout is the cost function J is not very well defined, so it's hard to calculate the cost\n",
    "     m) Data argument (flipping photos, random cropping) can be used as regularization technique\n",
    "     n) Early stoppingplot both dev set and training set cost vs. iteration, find the number that optimize both\n",
    "     o) Orthogonalization - separate the tasks of cost function optimization and reducing overfitting\n",
    "\n",
    "     Setting up your optimization problem\n",
    "     p) Normalizing training dataset; using unnormalized data, w1 and w2 might have very different values, increasing gradient steps needed before reaching optimal point\n",
    "     q) activation decreasing exponentially as the depth gets higher\n",
    "     r) In ReLU, use W = np.random.randn(shape)*np.sqrt(2/n[l-1])[He nomalization], might work better in initialization\n",
    "     s) Gradient checking: checking gradient using both side of the seta(w,b) value. gradient should not move too fast, otherwise bugged\n",
    "     t) Use gradient check only for debugging, not for training. remember regularization term; dropout does not work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Week 6\n",
    "     Optimization algorithms\n",
    "     a) mini-batch X{t}, mini batch t for input\n",
    "     b) Batch mode, all of your samples through 1 pass of many iterations; mini-batch, b (b< sample size) epochs through multiple iterations.\n",
    "     c) mini-batch size of 1 is called stochastic gradient descent, which will never converge\n",
    "     d) try to do mini batch size of 2^n, which allows the computer to run faster fit better to the CPU/GPU memory\n",
    "     e) Exponentially weighted averages (EWA): v_t = beta * v_t-1 + (1-beta) * seta_t, beta == 0.9 is appox. average over 10 units b/c 0.9^10 ~= 1/e, while beta==0.98 corresponding to 50 units.\n",
    "     f) Bias correction: Implementing EWA result in the initial low average values because no values for the           first a few time units. Using a bias correction can fix this vt/(1-beta^t) ; Not quite useful, can just ignore the first few samples\n",
    "     g) Gradient descent with momentum allow each iteration take advantage of previous step and moving towards the convex point faster: vdW = beta * vdw + (1-beta) * dW and use vdW to replace dW in backprop\n",
    "     h) RMSprop: Similar to previous one except SdW = beta * SdW + (1-beta) * dW^2,\n",
    "          W = W - sigma(dW/sqrt(SdW+epsilon)) epsilon = 10^-8 to avoid SdW==0 problem\n",
    "     i) Both Gradient descent with momentum and RMSprop allow the prop towards convex faster along the direction with longest distance and slower along the short distance\n",
    "     j) Adam optimization algorithm combines these two prop methods, commonly used\n",
    "     k) Hyper parameters in Adam optimization algorithm:\n",
    "         learning rate: need to be tuned, beta1: 0.9; beta2: 0.999; epsilon: 10^-8.\n",
    "         Adam: Adaptive moment estimation\n",
    "     l) Learning rate decay: exponential decay and discrete staircase; automatic or \"baby-sitting\"\n",
    "     j) Saddle point is more likely the optima for multi- dimensional space, rather than local minimal as optima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Week 7\n",
    "     Hyper parameters tuning\n",
    "     a) Hyper parameters: alpha-learning rate is the most import hyper parameter to tune\n",
    "     b) Random choose hyperparameter points, avoid iterating on un-important ones\n",
    "     c)  for alpha search log scale(alpha)\n",
    "     d)  for beta, search log scale (1-beta); using log scale is to take into account of sensitivity of value change at different end of hyperparameter range\n",
    "\n",
    "     Batch normalization\n",
    "     e) Batch normalization: makes training parameters much easier and faster, by normalize each l-1 layer Z before feed into layer l\n",
    "     f) In batch normal, no need to have parameter b[l], and replaced by beta[l], for b[l] will be subtract out when computing mean\n",
    "     g) Covariance shift, if distribution of X changes, that means you need redo the training\n",
    "     h) in deep network, distribution for hidden layer input a[i] keep changing, such layer i+1 suffer from covariance change\n",
    "     i) Each mini-batch scaled slightly different, adding noise to Z[i], similar to dropout function, it has slight regularization effect\n",
    "\n",
    "     Muti-class classification\n",
    "     j) Softmax layer embedded - softmax activation function; t = exp(Z[l]); a[l] = ti/sum(ti); a[l] = n by 1 vector\n",
    "     k) Soft max export probabilities rather than binary data like in \"hard max\" export\n",
    "\n",
    "     Programming framework\n",
    "     l) Tensorflow examples in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
