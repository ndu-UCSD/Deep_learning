{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure you ML project\n",
    "     - Week 8\n",
    "     a) Orthogonalization - Tuning can be conducted independently\n",
    "    Setting up your goal\n",
    "     b) Single number evaluation metic - Precision: recognized cats are cats, same as Bayes Law;      Recall: % actual cats being recognized\n",
    "     c) F1 score = harmonic mean of precision and recall; 2/(1/P+1/R), Choose classifier with higher F1 score. Alternatively, pick classifier with lower average error value\n",
    "     d) Repeat here, all dev and test data set should from the same distribution\n",
    "     e) Test may not be necessary, however highly recommended\n",
    "     f)  Add weight to critical errors that would filter out the unacceptable classifiers\n",
    "     Comparing to human level performance\n",
    "     g) Bayes optimal error: ML accuracy surpass human level performance rapidly but hard to exceed Bayes optimal error near 0\n",
    "     h) Before ML surpass human, you can get label data from humans, gain insight from manual error analysis, and get better analysis of bias and variance\n",
    "     i) Understanding HLP helps make decisions on whether to focus on bias or on variance\n",
    "     j) Human level error is usually defined based on the scenarios, and it is used as a proxy for Bayes error\n",
    "     k) Different between Training error and human level error is called \"avoidable bias\", whereas difference between training error and Dev error is \"variance\"\n",
    "     l) For structured data application, many ML algorithm surpassed HLP significantly\n",
    "\n",
    "Interview - Andrej Karpathy - Director of AI at Tesla; Autopilot, Young guy, 1987 or 88\n",
    "\n",
    "     - Week 9\n",
    "     Error analysis\n",
    "     a) Get the mislabeled samples, put into dev set examples\n",
    "     b) Debugging process should be focused on Type I andd Type II errors\n",
    "     c) With mislabeled examples, Deep Learning algorithms  are quite robust about random errors but not systematic errors\n",
    "     d) Approaches on how to conduct a DL project: Build system quickly, and then iterate\n",
    "     e) Set up dev/test set and metric -> build initial system quickly -> Use bias/variance analysis & error analysis to prioritize next step\n",
    "     Mismatched training and dev/test set\n",
    "     f) Data with different distribution - combine them and shuffle, however if data with different size can be problematic\n",
    "     g) Alternative, to include all data in training set, then dev/test with only data from small data set, if the small data set scenario is what we focus on, e.g. mobile app uploaded photos (and HD web photos for training)\n",
    "     h) Problem with this kind of data is that it's not certain if the increase in Training set to Dev set error is due to variance or not: Design a training - Dev set that the training - dev distribution is the same as training set, then the difference between training set <-> training-dev set, and training set <-> dev set can really tell if the error is from variance or distribution difference\n",
    "     i) Human error (Bayes error) ->(avoidable bias)-> Training set error -> (Variance) ->Training-dev error -> (data mis-match) ->dev error-> (degree of overfitting to the dev set) -> test error\n",
    "     j) Using artifical synthesis data to boost up data training, but careful with overfitting problem as the synthesized data might only represent a small portion of the overall data set.\n",
    "     Learning from multiple tasks - transfer learning\n",
    "     k) Transfer the knowledge of one task and use that to train another task; Initialize the last layer's weights, if small dataset, retrain the last a few layers, or with big data re-train all layers. Pre-training (task 1) and fine tuning (task 2)\n",
    "     l) I you have a lot of data for the initial model that enable you to characterize the fine details for recognition,\n",
    "     m) multi-tasks learning: output multiple labels; in back prop, sum up binary labels (different from softmax that has weights)\n",
    "     n) Use big NN for the multi-tasks learning, otherwise it hurts the performance; you can do all tasks at the same time but consider each NN for one specific task (transfer learning used more often than multi-tasks learning, except computer vision tasks)\n",
    "     End-to-end deep learning\n",
    "     o) More steps if using a smaller dataset, e.g. feature extractions; larger data set can be more straight forward using deep NN\n",
    "     p) Separate tasks by logic, e.g. face recognition -> 1. find face and corp to same size, and 2. compare face to database. Such the amount of data needed for training can be reduced significantly\n",
    "     q) Whether to use end-to-end approach? Pros - let data do the job and less hand design component needed; Cons - need large data, sometimes hand-design components could be really useful. Key! Do you have sufficient data to map from x to y?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
