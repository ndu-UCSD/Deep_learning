{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network and Deep learning \n",
    "     - Week 1\n",
    "     a) each neuron represent a prediction function - e.g. rectified linear unit (ReLU)\n",
    "     b) each neuron can be a specific ReLU function that represent a specific independent var in relation to Y\n",
    "     c) Real Estate - commonly use std NN; Image recognition - CNN; Sequence, audio recognition - RNN; auto-drive - custom complex NN\n",
    "     d) Structured data - tables; Unstructured data - audio or image\n",
    "     e) larger scale NN takes advantage of the larger amount of data\n",
    "     f) sigmoid function has gradient has slope close to 0 in the high and low end, makes the learning very slow; ReLU works much better\n",
    "\n",
    "Geoffrey Hinton interview - UCSD!! \"Invented\" back prop\n",
    "\n",
    "     - Week 2\n",
    "     a) Binary classification problem 1(Y) and 0(N)\n",
    "     b) Unroll pixel values to feature vector (include all values row* col* n)\n",
    "     c) x collections of feature vectors, y labels for collections\n",
    "     d) Logistic regression for binary problem: Given x, find y_hat = P{y=1|x}  # y=1 given x, 0<=y_hat<=1\n",
    "     e) One step of backward propagation on a computation graph yields derivative of a final output variable\n",
    "     f) Vectorization: W * xT, in python z=np.dot(W,x)+b. Transferring for loop into matrix calculation\n",
    "     g) Broadcasting in python allow computations via vectorization; (1,n) or (m,1) matrix will automatically expand to (m,n)\n",
    "     h) In python specify matrix structure, do not use rank 1 array (n,), do (n,1)\n",
    "\n",
    "Pieter Abbeel interview - Deep reinforcement learning\n",
    "\n",
    "     - Week 3\n",
    "     a) a[1](i) propagate to the next layer as input; a[2](i) = w[2](i).a[1](i)+b[2]\n",
    "     b) sigmold g'(z=0)~=1/4, tanh'(z=0)~=1; ReLU g'(z>=0) = 1 g'(z<0) = 0\n",
    "Ian Goodfellow interview - GAN\n",
    "\n",
    "     - Week 4\n",
    "     a) Vectorized calculation can only applied to one iteration in one layer\n",
    "     b) Parameter shapes: W[i]:(n[i],n[i-1]);      b[i]:(n[i],1);      dW[i]:(n[i],n[i-1]);     db[i]:(n[i],1);      A[i]:(n[i],m);     Z[i]:(n[i],m)\n",
    "     c) b[i] detention will be expanded during Z calculation due to boardcasting in python\n",
    "     d) Z[l] = W[l].A[l-1]+b[l]\n",
    "     e) forward a[l-1]->neuron->a[l]+cached Z[l] include W[l] and b[l];     backward da[i]->neuron->da[l-1] + cached dZ[l] include dW[l] and db[l]\n",
    "     f) Hyperparameters control parameters, they determine the values final parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic machine learning concept regardless of network structure\n",
    "####  Forward and Backward propagation\n",
    "Forward Propagation:\n",
    "- You get X\n",
    "- You compute $\\hat{Y} = A = \\sigma(w^T X + b) = (a^{(0)}, a^{(1)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n",
    "\n",
    "Here are the two formulas you will be using: \n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})$$\n",
    "\n",
    "####  Optimization\n",
    "- You have initialized your parameters.\n",
    "- You are also able to compute a cost function and its gradient.\n",
    "- Now, you want to update the parameters using gradient descent.\n",
    "\n",
    "$w$ and $b$ by minimizing the cost function $J$. For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Neural Network model architecture\n",
    "\n",
    "Neural Network with a single hidden layer.\n",
    "\n",
    "**Here is our model**:\n",
    "<img src=\"images/classification_kiank.png\" style=\"width:600px;height:300px;\">\n",
    "\n",
    "**Mathematically**:\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1] (i)}\\tag{1}$$ \n",
    "$$a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}$$\n",
    "$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2] (i)}\\tag{3}$$\n",
    "$$\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}$$\n",
    "$$y^{(i)}_{prediction} = \\begin{cases} 1 & \\mbox{if } a^{[2](i)} > 0.5 \\\\ 0 & \\mbox{otherwise } \\end{cases}\\tag{5}$$\n",
    "\n",
    "**Given the predictions on all the examples, you can also compute the cost**:\n",
    "<img src=\"images/J_function.png\" style=\"width:600px;height:50px;\">\n",
    "\n",
    "**Reminder**: The general methodology to build a Neural Network is to:\n",
    "    1. Define the neural network structure ( # of input units,  # of hidden units, etc). \n",
    "    2. Initialize the model's parameters\n",
    "    3. Loop:\n",
    "        - Implement forward propagation\n",
    "        - Compute loss\n",
    "        - Implement backward propagation to get the gradients\n",
    "        - Update parameters (gradient descent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
